version: '3.8'

services:
  # MCP Server - runs the actual Knowledge Graph MCP server over HTTP
  mcp-server:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: mcp-server
    environment:
      - WEAVIATE_HOST=weaviate
      - WEAVIATE_PORT=8080
      - WEAVIATE_GRPC_PORT=50051
      - PORT=4000
      # Specify your repository - change this to your repo
      - REPO_URL=${REPO_URL:-https://github.com/huggingface/transformers}
      # Or use a local path instead:
      # - REPO_PATH=/data/repo
    env_file:
      - .env
    depends_on:
      weaviate:
        condition: service_healthy
    volumes:
      - ./pedagogia_graph_code_repo:/app
      - ./pedagogia_graph_code_repo/data:/data
    ports:
      - "4000:4000"
    command: python run_mcp_server.py
    stdin_open: true
    tty: true
    healthcheck:
      test: [ "CMD-SHELL", "curl -f -s -o /dev/null -w '%{http_code}' http://localhost:4000/health | grep -q '^200$'" ]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 180s
    networks:
      - kg-network

  # Weaviate Vector Database
  weaviate:
    image: cr.weaviate.io/semitechnologies/weaviate:1.28.10
    container_name: weaviate
    ports:
      - "8080:8080"
      - "50051:50051"
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      DEFAULT_VECTORIZER_MODULE: 'none'
      ENABLE_MODULES: ''
      CLUSTER_HOSTNAME: 'node1'
    volumes:
      - weaviate_data:/var/lib/weaviate
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8080/v1/.well-known/ready" ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - kg-network

  # Gradio MCP Interface - direct interface to the MCP server
  gradio-mcp:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: gradio-mcp
    environment:
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
    env_file:
      - .env
    volumes:
      - .:/app
    ports:
      - "7860:7860"
    command: >
      python gradio_mcp.py
      --graph-file /data/knowledge_graph.pkl
      --host 0.0.0.0
      --port 7860
    stdin_open: true
    tty: true
    restart: unless-stopped
    networks:
      - kg-network
    depends_on:
      weaviate:
        condition: service_healthy

  # Smolagent Chat Interface - AI-powered chat interface
  smolagent-chat:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: smolagent-chat
    environment:
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      - MCP_SERVER_URL=http://mcp-server:4000/mcp
      - MAX_STEPS=5
      # OpenAI Configuration (loaded from .env file)
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_BASE_URL=${OPENAI_BASE_URL:-https://api.openai.com/v1}
      - OPENAI_MODEL_NAME=${OPENAI_MODEL_NAME:-gpt-4o-mini}
      - OPENAI_API_VERSION=${OPENAI_API_VERSION:-2024-02-15-preview}
    env_file:
      - .env
    depends_on:
      mcp-server:
        condition: service_healthy
      weaviate:
        condition: service_healthy
    volumes:
      - .:/app
    ports:
      - "7861:7861"
    command: >
      python smolagent_chat.py
      --mcp-server-url http://mcp-server:4000/mcp
      --host 0.0.0.0
      --port 7861
    stdin_open: true
    tty: true
    restart: unless-stopped
    networks:
      - kg-network

networks:
  kg-network:
    driver: bridge

volumes:
  weaviate_data:
