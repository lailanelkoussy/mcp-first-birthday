services:
  # Gradio MCP Interface - runs the MCP server with Gradio UI
  gradio-mcp:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: gradio-mcp
    environment:
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      # Specify your repository - change this to your repo
      - REPO_URL=${REPO_URL:-https://github.com/huggingface/transformers}
    volumes:
      - ./gradio_mcp.py:/app/gradio_mcp.py:ro
      - ./pedagogia_graph_code_repo:/app/pedagogia_graph_code_repo:ro
    ports:
      - "7860:7860"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7860/"]
      interval: 10s
      timeout: 15s
      retries: 5
      start_period: 120s
    stdin_open: true
    tty: true
    restart: unless-stopped
    networks:
      - kg-network

  # Smolagent Chat Interface - AI-powered chat interface that connects to gradio-mcp's MCP server
  smolagent-chat:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: smolagent-chat
    environment:
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      - MCP_SERVER_URL=http://gradio-mcp:7860/gradio_api/mcp/
      - MAX_STEPS=5
      # OpenAI Configuration (loaded from .env file)
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_BASE_URL=${OPENAI_BASE_URL:-https://api.openai.com/v1}
      - OPENAI_MODEL_NAME=${OPENAI_MODEL_NAME:-gpt-4o-mini}
      - OPENAI_API_VERSION=${OPENAI_API_VERSION:-2024-02-15-preview}
    depends_on:
      gradio-mcp:
        condition: service_healthy
      vllm:
        condition: service_healthy
    volumes:
      - .:/app
    ports:
      - "7861:7861"
    command: >
      python smolagent_chat.py
      --mcp-server-url http://gradio-mcp:7860/gradio_api/mcp/
      --host 0.0.0.0
      --port 7861
    stdin_open: true
    tty: true
    restart: unless-stopped
    networks:
      - kg-network

  

  vllm:
    image: vllm/vllm-openai:v0.11.0
    container_name: vllm
    ports:
      - "8000:8000"
    environment:
      VLLM_MODEL: HuggingFaceTB/SmolLM3-3B   # <--- change to your model
      #VLLM_API_KEY: "none"               # not required, but prevents warnings
    env_file:
      - .env
    command: >
      --model HuggingFaceTB/SmolLM3-3B
      --host 0.0.0.0
      --port 8000
    #--max-model-len 15000
    volumes:
      - ./models:/models
    networks:
      - kg-network
    # GPU SUPPORT (uncomment if needed)
    deploy:
      resources:
         reservations:
           devices:
             - driver: nvidia
               count: all
               capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v1/models" ]
      interval: 30s
      timeout: 20s
      retries: 10
      start_period: 60s

networks:
  kg-network:
    driver: bridge
